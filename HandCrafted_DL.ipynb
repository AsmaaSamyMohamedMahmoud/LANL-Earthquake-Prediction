{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Model2_less features.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "irGioDfUt3Mo",
        "outputId": "ed01bc80-2356-418b-f4f8-bff549bee6f9"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['test', 'train.csv', 'sample_submission.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QqqyGVegt3Mw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import NuSVR\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "mw5IrDfUt3Mw"
      },
      "source": [
        "train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SDDCOtskt3Mx",
        "outputId": "1b5c2e14-e5c9-456c-c794-b5e9119d998f"
      },
      "source": [
        "train [0:150000, 0 ] .mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.8841133"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "h0gcjeD2t3My"
      },
      "source": [
        "# pandas doesn't show us all the decimals\n",
        "pd.options.display.precision = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w4cqxbEst3My",
        "outputId": "2d371423-cc51-4621-af56-fcf3a021fdc8"
      },
      "source": [
        "rows = 150_000\n",
        "segments = int(np.floor(train.shape[0] / rows))\n",
        "print('train.shape',train.shape)\n",
        "segments\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.shape (629145480, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo70ASkMt3Mz"
      },
      "source": [
        "# **Extracting Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Vt9bzqbSt3Mz"
      },
      "source": [
        "def extract_features(z):\n",
        "     return np.c_[z.mean(axis=0), \n",
        "                  z.std(axis=0),\n",
        "                  z.max(axis=0),\n",
        "                  z.min(axis=0),\n",
        "                #  np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=0)) .reshape(1,4)\n",
        "                 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QHCcBeTbt3Mz"
      },
      "source": [
        "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
        "    if last_index == None:\n",
        "        last_index=len(x)\n",
        "       \n",
        "    assert last_index - n_steps * step_length >= 0\n",
        "\n",
        "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
        "    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n",
        "    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
        "    # MY CHANGE: This doesn't fix things, I get the same errors\n",
        "    temp = (x[(int(last_index) - n_steps * step_length):int(last_index)].reshape(n_steps,step_length ).astype(np.float32) - 5 ) / 3\n",
        "    \n",
        "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
        "    # of the last 10 observations. \n",
        "    return np.c_[extract_features(temp),\n",
        "                 extract_features(temp [ -n_steps // 10:]),\n",
        "                 extract_features(temp [ -n_steps // 100:])]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYL1Rb4t3Mz"
      },
      "source": [
        "# Creating Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8VQ4NuEWt3Mz",
        "outputId": "8f5f7935-c278-45a9-bf9c-8f9d32f1d6ac"
      },
      "source": [
        "# Query \"create_X\" to figure out the number of features\n",
        "n_features = create_X(train [0:150000,0]).shape[1]\n",
        "print(\"Our RNN is based on %i features\"% n_features)     # 18 features each row of segment ie 150x18 features of 150000 chunk input\n",
        "\n",
        "n_steps=150\n",
        "step_length=1000\n",
        "maxsize=train .shape[0]\n",
        "seg = int(np.floor(maxsize / (n_steps*step_length))) \n",
        "batch_size = seg-1   # (4193,) \n",
        "xx=350\n",
        "\n",
        "\n",
        "##############################################################################################\n",
        "rows_initialize = np.zeros((seg), dtype=float)\n",
        "print(rows_initialize.shape)\n",
        "\n",
        "for seg1 in tqdm(range(1,seg)) :      # for loop from 1 to 4194 segment value\n",
        "    rows_initialize [seg1] = seg1 * (n_steps*step_length) \n",
        "\n",
        "rows=np.delete(rows_initialize,0)    # (4193,)\n",
        "\n",
        "print(rows.shape)\n",
        "\n",
        "########################################################################################\n",
        "batch_size=batch_size-xx    # training data\n",
        "#batch_size=xx              # validation data\n",
        "split_point=xx\n",
        "second_earthquake = rows[xx]\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "if batch_size < 1000  :    # validation set \n",
        "               rows_1 = rows[:split_point+1]    #  0:350 \n",
        "        \n",
        "if batch_size > 1000 :   # training set\n",
        "               rows_1 = rows[split_point+1 :]    # (351,) ie 351:4193    \n",
        "            \n",
        "\n",
        "       \n",
        "    # Initialize feature matrices and targets\n",
        "samples_tr= np.zeros((rows_1.shape[0], step_length, n_features), dtype=float)   # (16, 150, 18)  for validation (350, 1, 24)  for training ( 3843, 1, 24) \n",
        "targets_tr = np.zeros(rows_1.shape[0], )    # (16,)  for validation (350)    for training ( 3843)\n",
        "        \n",
        "for j, row in enumerate(rows_1):             # 16 for validation (350)    for training ( 3843)\n",
        "    samples_tr[j] = create_X(train[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
        "    targets_tr[j] = train[int(row - 1), 1]         \n",
        "    \n",
        "    \n",
        "################################################################################################\n",
        "\n",
        "print('samples_tr shape', samples_tr.shape)\n",
        "print('targets_tr shape', targets_tr.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4193/4193 [00:00<00:00, 1146835.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Our RNN is based on 12 features\n",
            "(4194,)\n",
            "(4193,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "samples_tr shape (3842, 1000, 12)\n",
            "targets_tr shape (3842,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFsmQFcZt3M0"
      },
      "source": [
        "# Creating Validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xku-wOqnt3M0",
        "outputId": "f7426d18-71b5-4a99-bb58-bfd31e52b32d"
      },
      "source": [
        "#batch_size=batch_size-xx    # training data\n",
        "batch_size=xx              # validation data\n",
        "split_point=xx\n",
        "second_earthquake = rows[xx]\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "if batch_size < 1000  :    # validation set \n",
        "               rows_1 = rows[:split_point+1]    #  0:350 \n",
        "        \n",
        "if batch_size > 1000 :   # training set\n",
        "               rows_1 = rows[split_point+1 :]    # (351,) ie 351:4193    \n",
        "            \n",
        "       \n",
        "    # Initialize feature matrices and targets\n",
        "samples_vd= np.zeros((rows_1.shape[0], step_length, n_features), dtype=float)   #   for validation (350, 1, 24) \n",
        "targets_vd = np.zeros(rows_1.shape[0], )    #  for validation (350)    \n",
        "        \n",
        "for j, row in enumerate(rows_1):             # 16 for validation (350)    for training ( 3843)\n",
        "    samples_vd[j] = create_X(train[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
        "    targets_vd[j] = train[int(row - 1), 1]  \n",
        "    \n",
        "    \n",
        "print('samples_vd shape', samples_vd.shape)\n",
        "print('targets_vd shape',targets_vd.shape)  \n",
        "#print('rows_1 shape',rows_1.shape[0])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "samples_vd shape (351, 1000, 12)\n",
            "targets_vd shape (351,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mp1ceqtt3M1"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "buWsiYDst3M1",
        "outputId": "423d2c2a-d9bb-4d4a-80c1-1db166f8a528"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, CuDNNGRU, SimpleRNN, LSTM ,  Dropout, Activation, Flatten, Input, Conv1D, MaxPooling1D\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import RMSprop\n",
        "import datetime\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rJ1BGbY4npe"
      },
      "source": [
        "import keras "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3G3vzGTMt3M1"
      },
      "source": [
        "now = datetime.datetime.now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5GkZSr_Rt3M2"
      },
      "source": [
        "## CNN combined with LSTM Model \n",
        "i = (1000, 12)\n",
        "model = Sequential ()\n",
        "##1st model\n",
        "model.add(Conv1D(5, 3, activation='relu', input_shape= i))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(LSTM(50,  return_sequences=True))\n",
        "model.add(LSTM(10))\n",
        "model.add(Dense(240))\n",
        "model.add(Dense(120))\n",
        "model.add(Dense(60))\n",
        "model.add(Dense(30))\n",
        "model.add(Dense(1))\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ai6bK0OAt3M2",
        "outputId": "dddafaa7-c4d6-4390-dbe1-2a4e96bd231e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_2 (Conv1D)            (None, 998, 5)            185       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 499, 5)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 499, 50)           11200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 10)                2440      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 240)               2640      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 120)               28920     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 60)                7260      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 54,506\n",
            "Trainable params: 54,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnXvoHOft3M2"
      },
      "source": [
        "# Compile and fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9M2bOlaIt3M3",
        "outputId": "99d7c92c-44be-4496-f4a6-a21475776985"
      },
      "source": [
        "import keras\n",
        "from keras.optimizers import RMSprop\n",
        "opt = keras.optimizers.adam(lr=.005)\n",
        "\n",
        "model.compile(loss=\"mae\",\n",
        "              optimizer=opt, metrics=['mean_absolute_error'])\n",
        "             # metrics=['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 128 # mini-batch with 32 examples\n",
        "epochs = 50\n",
        "t=now()\n",
        "history = model.fit(\n",
        "    samples_tr, targets_tr,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(samples_vd  ,targets_vd ))\n",
        "print('Training time: %s' % (now() - t))       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 3842 samples, validate on 351 samples\n",
            "Epoch 1/50\n",
            "3842/3842 [==============================] - 63s 16ms/step - loss: 3.0513 - mean_absolute_error: 3.0513 - val_loss: 2.7415 - val_mean_absolute_error: 2.7415\n",
            "Epoch 2/50\n",
            "3842/3842 [==============================] - 56s 15ms/step - loss: 2.7165 - mean_absolute_error: 2.7165 - val_loss: 2.8408 - val_mean_absolute_error: 2.8408\n",
            "Epoch 3/50\n",
            "3842/3842 [==============================] - 54s 14ms/step - loss: 2.5214 - mean_absolute_error: 2.5214 - val_loss: 2.6144 - val_mean_absolute_error: 2.6144\n",
            "Epoch 4/50\n",
            "3842/3842 [==============================] - 55s 14ms/step - loss: 2.3952 - mean_absolute_error: 2.3952 - val_loss: 2.6397 - val_mean_absolute_error: 2.6397\n",
            "Epoch 5/50\n",
            "3842/3842 [==============================] - 54s 14ms/step - loss: 2.4246 - mean_absolute_error: 2.4246 - val_loss: 2.3496 - val_mean_absolute_error: 2.3496\n",
            "Epoch 6/50\n",
            "3842/3842 [==============================] - 54s 14ms/step - loss: 2.2929 - mean_absolute_error: 2.2929 - val_loss: 2.2519 - val_mean_absolute_error: 2.2519\n",
            "Epoch 7/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.2876 - mean_absolute_error: 2.2876 - val_loss: 2.6121 - val_mean_absolute_error: 2.6121\n",
            "Epoch 8/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.2675 - mean_absolute_error: 2.2675 - val_loss: 2.1405 - val_mean_absolute_error: 2.1405\n",
            "Epoch 9/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.2707 - mean_absolute_error: 2.2707 - val_loss: 2.5129 - val_mean_absolute_error: 2.5129\n",
            "Epoch 10/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.4190 - mean_absolute_error: 2.4190 - val_loss: 2.2702 - val_mean_absolute_error: 2.2702\n",
            "Epoch 11/50\n",
            "3842/3842 [==============================] - 52s 14ms/step - loss: 2.3690 - mean_absolute_error: 2.3690 - val_loss: 2.5099 - val_mean_absolute_error: 2.5099\n",
            "Epoch 12/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.2717 - mean_absolute_error: 2.2717 - val_loss: 2.3554 - val_mean_absolute_error: 2.3554\n",
            "Epoch 13/50\n",
            "3842/3842 [==============================] - 53s 14ms/step - loss: 2.2835 - mean_absolute_error: 2.2835 - val_loss: 2.3395 - val_mean_absolute_error: 2.3395\n",
            "Epoch 14/50\n",
            "3842/3842 [==============================] - 52s 13ms/step - loss: 2.2983 - mean_absolute_error: 2.2983 - val_loss: 2.5022 - val_mean_absolute_error: 2.5022\n",
            "Epoch 15/50\n",
            "3842/3842 [==============================] - 52s 14ms/step - loss: 2.3044 - mean_absolute_error: 2.3044 - val_loss: 2.3897 - val_mean_absolute_error: 2.3897\n",
            "Epoch 16/50\n",
            "3842/3842 [==============================] - 52s 14ms/step - loss: 2.3483 - mean_absolute_error: 2.3483 - val_loss: 2.1742 - val_mean_absolute_error: 2.1742\n",
            "Epoch 17/50\n",
            "3842/3842 [==============================] - 52s 13ms/step - loss: 2.2404 - mean_absolute_error: 2.2404 - val_loss: 2.4914 - val_mean_absolute_error: 2.4914\n",
            "Epoch 18/50\n",
            "3842/3842 [==============================] - 52s 14ms/step - loss: 2.2978 - mean_absolute_error: 2.2978 - val_loss: 2.1773 - val_mean_absolute_error: 2.1773\n",
            "Epoch 19/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.3442 - mean_absolute_error: 2.3442 - val_loss: 2.1737 - val_mean_absolute_error: 2.1737\n",
            "Epoch 20/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.3315 - mean_absolute_error: 2.3315 - val_loss: 2.2889 - val_mean_absolute_error: 2.2889\n",
            "Epoch 21/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.2743 - mean_absolute_error: 2.2743 - val_loss: 2.2529 - val_mean_absolute_error: 2.2529\n",
            "Epoch 22/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.3067 - mean_absolute_error: 2.3067 - val_loss: 2.2239 - val_mean_absolute_error: 2.2239\n",
            "Epoch 23/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2338 - mean_absolute_error: 2.2338 - val_loss: 2.1928 - val_mean_absolute_error: 2.1928\n",
            "Epoch 24/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.2362 - mean_absolute_error: 2.2362 - val_loss: 2.7882 - val_mean_absolute_error: 2.7882\n",
            "Epoch 25/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.3806 - mean_absolute_error: 2.3806 - val_loss: 2.4131 - val_mean_absolute_error: 2.4131\n",
            "Epoch 26/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.3000 - mean_absolute_error: 2.3000 - val_loss: 2.3675 - val_mean_absolute_error: 2.3675\n",
            "Epoch 27/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.3641 - mean_absolute_error: 2.3641 - val_loss: 2.0929 - val_mean_absolute_error: 2.0929\n",
            "Epoch 28/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.3387 - mean_absolute_error: 2.3387 - val_loss: 2.3516 - val_mean_absolute_error: 2.3516\n",
            "Epoch 29/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2304 - mean_absolute_error: 2.2304 - val_loss: 2.4153 - val_mean_absolute_error: 2.4153\n",
            "Epoch 30/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.2274 - mean_absolute_error: 2.2274 - val_loss: 2.3863 - val_mean_absolute_error: 2.3863\n",
            "Epoch 31/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2393 - mean_absolute_error: 2.2393 - val_loss: 2.4606 - val_mean_absolute_error: 2.4606\n",
            "Epoch 32/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.3094 - mean_absolute_error: 2.3094 - val_loss: 2.1548 - val_mean_absolute_error: 2.1548\n",
            "Epoch 33/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2961 - mean_absolute_error: 2.2961 - val_loss: 2.3746 - val_mean_absolute_error: 2.3746\n",
            "Epoch 34/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2456 - mean_absolute_error: 2.2456 - val_loss: 2.1191 - val_mean_absolute_error: 2.1191\n",
            "Epoch 35/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2585 - mean_absolute_error: 2.2585 - val_loss: 2.2733 - val_mean_absolute_error: 2.2733\n",
            "Epoch 36/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2440 - mean_absolute_error: 2.2440 - val_loss: 2.4173 - val_mean_absolute_error: 2.4173\n",
            "Epoch 37/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2476 - mean_absolute_error: 2.2476 - val_loss: 2.1536 - val_mean_absolute_error: 2.1536\n",
            "Epoch 38/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.3046 - mean_absolute_error: 2.3046 - val_loss: 2.0244 - val_mean_absolute_error: 2.0244\n",
            "Epoch 39/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2804 - mean_absolute_error: 2.2804 - val_loss: 2.3540 - val_mean_absolute_error: 2.3540\n",
            "Epoch 40/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2257 - mean_absolute_error: 2.2257 - val_loss: 2.4501 - val_mean_absolute_error: 2.4501\n",
            "Epoch 41/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2860 - mean_absolute_error: 2.2860 - val_loss: 2.5071 - val_mean_absolute_error: 2.5071\n",
            "Epoch 42/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.3218 - mean_absolute_error: 2.3218 - val_loss: 1.9240 - val_mean_absolute_error: 1.9240\n",
            "Epoch 43/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2702 - mean_absolute_error: 2.2702 - val_loss: 2.1915 - val_mean_absolute_error: 2.1915\n",
            "Epoch 44/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.3125 - mean_absolute_error: 2.3125 - val_loss: 2.3619 - val_mean_absolute_error: 2.3619\n",
            "Epoch 45/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2261 - mean_absolute_error: 2.2261 - val_loss: 2.3076 - val_mean_absolute_error: 2.3076\n",
            "Epoch 46/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2244 - mean_absolute_error: 2.2244 - val_loss: 2.1597 - val_mean_absolute_error: 2.1597\n",
            "Epoch 47/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2324 - mean_absolute_error: 2.2324 - val_loss: 2.2495 - val_mean_absolute_error: 2.2495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48/50\n",
            "3842/3842 [==============================] - 50s 13ms/step - loss: 2.2364 - mean_absolute_error: 2.2364 - val_loss: 2.1751 - val_mean_absolute_error: 2.1751\n",
            "Epoch 49/50\n",
            "3842/3842 [==============================] - 51s 13ms/step - loss: 2.2126 - mean_absolute_error: 2.2126 - val_loss: 2.3005 - val_mean_absolute_error: 2.3005\n",
            "Epoch 50/50\n",
            "3842/3842 [==============================] - 49s 13ms/step - loss: 2.2816 - mean_absolute_error: 2.2816 - val_loss: 2.4618 - val_mean_absolute_error: 2.4618\n",
            "Training time: 0:42:45.084169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "OTVAwQght3M3"
      },
      "source": [
        "# Load submission file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kHyBKNAzt3M3"
      },
      "source": [
        "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgZxGPmt3M4"
      },
      "source": [
        "## Prepare submission data\n",
        "Load each test data, create the feature matrix, get numeric prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Rtb4e5SPt3M4",
        "outputId": "43da340d-6ba2-43e4-9ad8-ea10c830ee7c"
      },
      "source": [
        "for i, seg_id in enumerate(tqdm(submission.index)):\n",
        "  #  print(i)\n",
        "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
        "    x = seg['acoustic_data'].values\n",
        "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
        "\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2624/2624 [28:32<00:00,  1.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              time_to_failure\n",
              "seg_id                       \n",
              "seg_00030f  5.116579055786133\n",
              "seg_0012b5  4.282918453216553\n",
              "seg_00184e  4.954693317413330\n",
              "seg_003339  7.273122310638428\n",
              "seg_0042cc  4.488538742065430"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_to_failure</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seg_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>seg_00030f</th>\n",
              "      <td>5.116579055786133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seg_0012b5</th>\n",
              "      <td>4.282918453216553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seg_00184e</th>\n",
              "      <td>4.954693317413330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seg_003339</th>\n",
              "      <td>7.273122310638428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seg_0042cc</th>\n",
              "      <td>4.488538742065430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "47K_2j9Bt3M4"
      },
      "source": [
        "submission.to_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}